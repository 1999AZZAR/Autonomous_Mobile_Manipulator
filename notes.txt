main body and nav:
    - 6x sensor jarak (laser based)(l,r,b)(2 per side)(used to alighn the robot with the wall)
    - 2x sensor jarak (hcsr04)(f)
    - 3x individual line sensor assembled side by side (used to align the robot with the line on the floor/ can also be used to navigate as navigate my line following mode)

    - 3x omni (lf,rf,b):
        - back
        - front left
        - front right

    - single poin lidar (tf_luma)
    - imu (mpu6050)

main gripper:
    - 1x motor (same motor used on omni)(move up down)
    - 2x servo (1 tilt, 1 open/close)
    - 1x 360degree (free rotation) servo (optional) (used to move the gripper asembly forward/backward)
    - 1 usb based camera (can move up down with the gripper mechanism)


containe load (to save piced object)(optional)
- 2 on left
    - left front
    - left back
- 2 on right
    - right front
    - right back


robot shape
- hexagonal shaped robot


hw controll
- emergency
- start/stop
- mode (train/run)


brain/controll
- raspiberry pi 5
    - ubuntu server
    - docker
    - tailscale
- n8n (on docker)
    - automation
- ros2 (on docker)
    - low level
    - direct io


contoll mechanism
- path planing
- automatic planing based on map data training
- obstacle avoidance
- line follower
- pid based contoll to gain smooth motor characteristic movement
- object pick and place
- object recongnition

ros2 api need tobe accesible all the time:
- imu pos
- robot log
- 3 last command/execution data

