main body and nav:
    - 6x sensor jarak (analog IR distance sensors)(l,r,b)(2 per side)(used to align the robot with the wall)(Sharp GP2Y0A02YK0F)
        - Connected via MCP3008 ADC (SPI interface)
        - Range: 20-150cm (200-1500mm)
        - Output: Analog voltage 0.4V-2.7V
    - 2x sensor jarak (hcsr04)(f)
    - 3x individual line sensor assembled side by side (used to align the robot with the line on the floor/ can also be used to navigate as navigate my line following mode)

    - 3x omni (lf,rf,b):
        - back
        - front left
        - front right

    - single poin lidar (tf_luna)(i2c/uart)
    - imu (mpu6050)

main gripper:
    - 1x motor (same motor used on omni)(move up down)
    - 2x servo (1 tilt, 1 open/close)
    - 1x 360degree (free rotation) servo (optional) (used to move the gripper asembly forward/backward)
    - 1 usb based camera (can move up down with the gripper mechanism)


containe load (to save piced object)(optional)
- 2 on left
    - left front
    - left back
- 2 on right
    - right front
    - right back


robot shape
- hexagonal shaped robot


hw controll
- emergency
- start/stop
- mode (train/run)


brain/controll
- raspiberry pi 5
    - ubuntu server
    - docker
    - tailscale
- n8n (on docker)
    - automation
- ros2 (on docker)
    - low level
    - direct io


contoll mechanism
- path planing
- automatic planing based on map data training
- obstacle avoidance
- line follower
- pid based contoll to gain smooth motor characteristic movement
- object pick and place
- object recongnition

ros2 api need tobe accesible all the time:
- imu pos
- robot log
- 3 last command/execution data

recent updates (2025-11-11):
- Fixed IMU data display in web interface
  - Enhanced error handling in read_imu_data() method
  - Added safe dictionary access with .get() and default values
  - Added debug logging for troubleshooting
  - Created test_imu_endpoint.py for API testing
  - Created IMU_TROUBLESHOOTING.md and QUICK_IMU_TEST.md guides
  - IMU sensor (MPU6050) verified working via direct test
  - Web interface now properly displays orientation, angular velocity, and acceleration data

- Made all scripts and documentation portable (no hardcoded paths)
  - Scripts use dynamic path resolution
  - Works on any machine/user/directory
  - Added README_SETUP.md explaining project root concept
  - Updated all documentation to use relative paths
  - Added path validation with helpful error messages
  - Created PORTABLE_PATHS_UPDATE.md documenting changes

- Added explicit hardware/simulation mode control
  - Web interface now accepts --hardware, --simulation, --auto flags
  - Created start_hardware.sh for real robot operation
  - Created start_simulation.sh for testing without hardware
  - Updated run.sh to set ROBOT_MODE environment variable
  - Added MODE_SELECTION.md comprehensive mode guide
  - Prevents web interface from getting stuck in simulation mode
  - Hardware mode requires I2C permissions and real sensors
  - Simulation mode works anywhere without hardware

- Added automatic workspace building
  - Created build_workspace.sh for first-time setup
  - Startup scripts now automatically build workspace if needed
  - Checks for ROS2, colcon, and Python dependencies
  - Installs missing Python packages automatically
  - Created FIRST_RUN.md complete setup guide from scratch
  - Scripts detect if workspace not built and offer to build it

